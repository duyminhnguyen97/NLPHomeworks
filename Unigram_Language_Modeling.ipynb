{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Unigram_Language_Modeling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/duyminhnguyen97/NLPHomeworks/blob/master/Unigram_Language_Modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6LHg063ia8q",
        "colab_type": "text"
      },
      "source": [
        "# Unigram Language Modeling\n",
        "\n",
        "In this document, we will learn:\n",
        "\n",
        "- How to implement an unigram language model on a training data\n",
        "- How to evaluate a language model on a test data using perplexity measure\n",
        "\n",
        "## Data\n",
        "\n",
        "We will use the file [wiki-en-train.word](https://raw.githubusercontent.com/neubig/nlptutorial/master/data/wiki-en-train.word) as the training data, and [wiki-en-test.\n",
        "word](https://raw.githubusercontent.com/neubig/nlptutorial/master/data/wiki-en-test.word) as the test data. To test our implementation quickly, we will use small data files [01-train-input.txt](https://github.com/neubig/nlptutorial/blob/master/test/01-train-input.txt) and [01-test-input.txt](https://github.com/neubig/nlptutorial/blob/master/test/01-test-input.txt). All data files are from the [nlptutorial](https://github.com/neubig/nlptutorial) by Graham Neubig.\n",
        "\n",
        "As the first step, we will download all necessary data files using `wget` command line.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8ts_vQ4mUr-",
        "colab_type": "code",
        "outputId": "20bebdd9-2069-4aeb-9a30-8831a03e8338",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 801
        }
      },
      "source": [
        "!rm -f wiki-en-train.word\n",
        "!wget https://raw.githubusercontent.com/neubig/nlptutorial/master/data/wiki-en-train.word\n",
        "    \n",
        "!rm -f wiki-en-test.word\n",
        "!wget https://raw.githubusercontent.com/neubig/nlptutorial/master/data/wiki-en-test.word\n",
        "\n",
        "!rm -f 01-train-input.txt\n",
        "!wget https://raw.githubusercontent.com/neubig/nlptutorial/master/test/01-train-input.txt\n",
        "\n",
        "!rm -f 01-test-input.txt\n",
        "!wget https://raw.githubusercontent.com/neubig/nlptutorial/master/test/01-test-input.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-01-18 04:42:09--  https://raw.githubusercontent.com/neubig/nlptutorial/master/data/wiki-en-train.word\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 203886 (199K) [text/plain]\n",
            "Saving to: ‘wiki-en-train.word’\n",
            "\n",
            "\rwiki-en-train.word    0%[                    ]       0  --.-KB/s               \rwiki-en-train.word  100%[===================>] 199.11K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2020-01-18 04:42:09 (7.75 MB/s) - ‘wiki-en-train.word’ saved [203886/203886]\n",
            "\n",
            "--2020-01-18 04:42:19--  https://raw.githubusercontent.com/neubig/nlptutorial/master/data/wiki-en-test.word\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 26989 (26K) [text/plain]\n",
            "Saving to: ‘wiki-en-test.word’\n",
            "\n",
            "wiki-en-test.word   100%[===================>]  26.36K  --.-KB/s    in 0.007s  \n",
            "\n",
            "2020-01-18 04:42:19 (3.68 MB/s) - ‘wiki-en-test.word’ saved [26989/26989]\n",
            "\n",
            "--2020-01-18 04:42:22--  https://raw.githubusercontent.com/neubig/nlptutorial/master/test/01-train-input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12 [text/plain]\n",
            "Saving to: ‘01-train-input.txt’\n",
            "\n",
            "01-train-input.txt  100%[===================>]      12  --.-KB/s    in 0s      \n",
            "\n",
            "2020-01-18 04:42:22 (2.28 MB/s) - ‘01-train-input.txt’ saved [12/12]\n",
            "\n",
            "--2020-01-18 04:42:26--  https://raw.githubusercontent.com/neubig/nlptutorial/master/test/01-test-input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6 [text/plain]\n",
            "Saving to: ‘01-test-input.txt’\n",
            "\n",
            "01-test-input.txt   100%[===================>]       6  --.-KB/s    in 0s      \n",
            "\n",
            "2020-01-18 04:42:26 (1.01 MB/s) - ‘01-test-input.txt’ saved [6/6]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiSRpxJhnN8R",
        "colab_type": "text"
      },
      "source": [
        "## Unigram Language Model\n",
        "\n",
        "We are going to implement unigram language model in this section. We will write two functions:\n",
        "\n",
        "- `train_unigram`: Creates a unigram model from the training data and save the model to a file.\n",
        "- `test-unigram`: Reads a unigram model and calculates entropy, perplexity and coverage for the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkG6IQEToENx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "def train_unigram(train_file, model_file):\n",
        "    counts = defaultdict(int)     # To count c(w_i)\n",
        "    total_count = 0  # to count total words\n",
        "    \n",
        "    with open(train_file, 'r') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line == '':\n",
        "                continue\n",
        "            words = line.split()\n",
        "            words.append('</s>')\n",
        "            for word in words:\n",
        "                counts[word] += 1\n",
        "                total_count += 1\n",
        "    with open(model_file, 'w') as fo:\n",
        "        for word, count in counts.items():\n",
        "            probability = counts[word]/total_count\n",
        "            fo.write('%s\\t%f\\n' % (word, probability))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c4GfvRbCCku",
        "colab_type": "text"
      },
      "source": [
        "Now let's test the function on the small data to verify that our implementation is correct."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j00v4g_p-f08",
        "colab_type": "code",
        "outputId": "5d32b681-a58f-405c-d6f5-e9c4c186a89f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "!cat 01-train-input.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a b c\n",
            "a b d\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fShIvdqCTOF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_unigram('./01-train-input.txt', '01-train-answer.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akDPba5ICaCk",
        "colab_type": "code",
        "outputId": "8ec959b4-6251-4f11-8431-aea118559e78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "!cat 01-train-answer.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a\t0.250000\n",
            "b\t0.250000\n",
            "c\t0.125000\n",
            "</s>\t0.250000\n",
            "d\t0.125000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Gj248iACt97",
        "colab_type": "text"
      },
      "source": [
        "We will now implement the function `test_unigram` for evaluating the model. We will need to load the model file before evaluating it on the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nlPk5LDEDLj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_unigram_model(model_file):\n",
        "    probabilities = {}\n",
        "    with open(model_file, 'r') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line == '':\n",
        "                continue\n",
        "            w, p = line.split()\n",
        "            probabilities[w] = float(p)\n",
        "    return probabilities"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txUsmB9PFP1c",
        "colab_type": "code",
        "outputId": "1ad6b14f-d5cb-4fcf-ea7d-752546db120e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "probabilities = load_unigram_model('01-train-answer.txt')\n",
        "print(probabilities)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'a': 0.25, 'b': 0.25, 'c': 0.125, '</s>': 0.25, 'd': 0.125}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSg9xSssFXRJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math \n",
        "\n",
        "\n",
        "lambda1 = 0.95\n",
        "lambda_unk = 1 - lambda1\n",
        "V = 1000000\n",
        "\n",
        "def test_unigram(test_file, model_file):\n",
        "    probabilities = load_unigram_model(model_file)\n",
        "    W = 0  # total words\n",
        "    H = 0  # entropy\n",
        "    unk = 0  # total unknown words\n",
        "    with open(test_file, 'r') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line == '':\n",
        "                continue\n",
        "            words = line.split()\n",
        "            words.append('</s>')\n",
        "            for w in words:\n",
        "                W += 1\n",
        "                p = lambda_unk/V\n",
        "                if w in probabilities:\n",
        "                    p += lambda1 *  probabilities[w]\n",
        "                else:\n",
        "                    unk += 1\n",
        "                H += -math.log2(p)\n",
        "    H = H/W\n",
        "    print('Entropy: {}'.format(H))\n",
        "    print('Perplexity: {}'.format(2**H))\n",
        "    print('Coverage: {}'.format((W-unk)/W))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wt_ekGpGHncE",
        "colab_type": "code",
        "outputId": "cd491bd6-4815-414b-ade1-33836bda06ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "test_unigram('01-test-input.txt', '01-train-answer.txt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Entropy: 6.709899494272102\n",
            "Perplexity: 104.6841703912115\n",
            "Coverage: 0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oKNC6LQH3bU",
        "colab_type": "text"
      },
      "source": [
        "Let's test unigram language model on the larger data files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZOWU7wGH8qS",
        "colab_type": "code",
        "outputId": "ce2fcde2-0a12-4ecf-912a-c821a4a86eac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "train_unigram('wiki-en-train.word', 'unigram_model.txt')\n",
        "test_unigram('wiki-en-test.word', 'unigram_model.txt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Entropy: 10.526656347101143\n",
            "Perplexity: 1475.1606346867834\n",
            "Coverage: 0.895226024503591\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrp_LN9g_1_b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}